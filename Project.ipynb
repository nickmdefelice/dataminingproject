{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickmdefelice/dataminingproject/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLXYX5EwU_4U"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "My code is fully functioning with smaller values of N for the training set, but when I make it bigger it often runs out of memory in Colab or crashes. I set N, which is the # of stocks, equal to 2000 by default but if it doesn't work, you can change the value of N to something smaller like 500 or less. It should still perform well. It works with the full testing sets though."
      ],
      "metadata": {
        "id": "YTvRSNXm7Zun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2000"
      ],
      "metadata": {
        "id": "Qwz9mjhb8QiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ5rpr_wQXZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pickle\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJG1h1Gi2d2M"
      },
      "source": [
        "**Task 1: Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAAPKkSZkwZf"
      },
      "source": [
        "Load training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XNzPCf-ntHP"
      },
      "outputs": [],
      "source": [
        "with open('training_set.pkl', 'rb') as f:\n",
        "    training_set = pickle.load(f)\n",
        "#Reads training_set.pkl and stores in training_set\n",
        "#training_set is a list of 2,000 pandas dataframes, each representing the historical trading data of a single stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMKhPrQHQiVK"
      },
      "outputs": [],
      "source": [
        "# Use only the first N stocks\n",
        "#N = 10\n",
        "training_set = training_set[:N]\n",
        "\n",
        "\n",
        "\n",
        "# Continue with your code, using 'smaller_training_set' instead of 'training_set'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMQTK2n0jS"
      },
      "source": [
        "Calculate the daily percentage change and flattens into 1D arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9mn8_oHn1Zm"
      },
      "outputs": [],
      "source": [
        "#price_changes = [stock['Close'].pct_change().dropna().tolist() for stock in training_set]\n",
        "flat_changes = [pct_change for stock in training_set for pct_change in stock['Close'].pct_change().dropna()]\n",
        "#Calculates daily percentage change in the closing prices for each stock in training_set\n",
        "#Calculated relative to previous day's closing price\n",
        "#Stored in price_changes list, which is a list of lists containing the % changes for each stock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XPW__cOoB18"
      },
      "source": [
        "Determine thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odggi7sZoCmG"
      },
      "outputs": [],
      "source": [
        "sorted_changes = np.sort(flat_changes)\n",
        "num_points = len(sorted_changes)\n",
        "num_third = num_points // 3\n",
        "threshold1 = sorted_changes[num_third]\n",
        "threshold2 = sorted_changes[num_third * 2]\n",
        "\n",
        "#Calculates two thresholds used to divide percentage changes into three levels: increase, decrease, and no change\n",
        "#Calculated by sorting flat_changes and selecting values at 1/3 and 2/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTNpOO6VoILR"
      },
      "source": [
        "Divide the changes into 3 levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSWp8SXFoI5w"
      },
      "outputs": [],
      "source": [
        "num_decrease = sum(pct_change < threshold1 for pct_change in flat_changes)\n",
        "num_nochange = sum(threshold1 <= pct_change < threshold2 for pct_change in flat_changes)\n",
        "num_increase = num_points - num_decrease - num_nochange\n",
        "\n",
        "#Iterates through price_changes and counts the # of % changes that falls into each level\n",
        "#Stored in num_increase, num_nochange, and num_decrease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2Hjs1XoUai"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEGtAGbFoWX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3431ad41-ac76-4c69-fe25-961e8d882ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold 1:  -0.001942987939540286\n",
            "Threshold 2:  0.0019470602816240579\n",
            "Number of data points for increase:  7338\n",
            "Number of data points for no change:  7336\n",
            "Number of data points for decrease:  7336\n"
          ]
        }
      ],
      "source": [
        "print(\"Threshold 1: \", threshold1)\n",
        "print(\"Threshold 2: \", threshold2)\n",
        "print(\"Number of data points for increase: \", num_increase)\n",
        "print(\"Number of data points for no change: \", num_nochange)\n",
        "print(\"Number of data points for decrease: \", num_decrease)\n",
        "#Prints the thresholds and the # for each level of % changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "809BcNsR2oVC"
      },
      "source": [
        "**Task 2: Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPvvvxv2m2n"
      },
      "source": [
        "Create features function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMc_Fx9e3T2S"
      },
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    # Feature 1: Daily Close Percentage Change\n",
        "    df['PCT_CHG'] = df['Close'].pct_change()\n",
        "\n",
        "    #Feature 2: Moving Average 5\n",
        "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
        "\n",
        "    # Feature 3: Moving Average 10\n",
        "    df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
        "\n",
        "    #Feature 4: Moving Average 20\n",
        "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "    #Feature 5: Moving Average 50\n",
        "    df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
        "\n",
        "    #Feature 6: Moving Average 100\n",
        "    df['MA_100'] = df['Close'].rolling(window=100).mean()\n",
        "\n",
        "    # Feature 7: Moving Average Convergence Divergence (MACD)\n",
        "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = exp1 - exp2\n",
        "\n",
        "    # Feature 8: Relative Strength Index (RSI)\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=14).mean()\n",
        "    avg_loss = loss.rolling(window=14).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    df['RSI'] = rsi\n",
        "\n",
        "    # Feature 9: Money Flow Index (MFI)\n",
        "    typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    money_flow = typical_price * df['Volume']\n",
        "    positive_flow = money_flow.where(df['PCT_CHG'] > 0, 0).rolling(window=14).sum()\n",
        "    negative_flow = money_flow.where(df['PCT_CHG'] < 0, 0).rolling(window=14).sum()\n",
        "    mfi = 100 - (100 / (1 + (positive_flow / negative_flow)))\n",
        "    df['MFI'] = mfi\n",
        "\n",
        "    # Feature 10: High minus Low\n",
        "    df['H-L'] = df['High'] - df['Low']\n",
        "\n",
        "    # Feature 11: High minus Close\n",
        "    df['H-C'] = df['High'] - df['Close']\n",
        "\n",
        "    # Feature 12: Low minus Close\n",
        "    df['L-C'] = df['Low'] - df['Close']\n",
        "\n",
        "    # Feature 13: Close minus Open\n",
        "    df['Close-Open'] = df['Close'] - df['Open']\n",
        "\n",
        "    #Feature 14: Log high\n",
        "    df['log_high'] = np.log(df['High'])\n",
        "\n",
        "    #Feature 15: Log low\n",
        "    df['log_low'] = np.log(df['Low'])\n",
        "\n",
        "    #Feature 16: Log close\n",
        "    df['log_close'] = np.log(df['Close'])\n",
        "\n",
        "    #Feature 17: Log volume\n",
        "    df['log_volume'] = np.log(df['Volume'])\n",
        "\n",
        "    #Features 18 and 19: Bollinger Bands\n",
        "    sma = df['Close'].rolling(window=20).mean()\n",
        "    std = df['Close'].rolling(window=20).std()\n",
        "    upper_band = sma + (2 * std)\n",
        "    lower_band = sma - (2 * std)\n",
        "    df['BB_upper'] = upper_band\n",
        "    df['BB_lower'] = lower_band\n",
        "\n",
        "    #Feature 20: Standard Deviation of Close\n",
        "    df['Std_Close'] = df['Close'].rolling(window=20).std()\n",
        "\n",
        "    #Feature 21: Mode of Close\n",
        "    df['Mode_Close'] = df['Close'].mode().iloc[0]\n",
        "\n",
        "    #Feature 22: Range of Close\n",
        "    df['Range_Close'] = df['High'] - df['Low']\n",
        "\n",
        "    #Feature 23: Sharpe Ratio\n",
        "    daily_returns = df['Close'].pct_change()\n",
        "    annualized_mean = daily_returns.mean() * 252\n",
        "    annualized_std = daily_returns.std() * np.sqrt(252)\n",
        "    df['Sharpe_Ratio'] = annualized_mean / annualized_std\n",
        "\n",
        "    #Feature 24: Volume Percent Change\n",
        "    df['Volume_Change'] = df['Volume'].pct_change()\n",
        "\n",
        "    #Feature 25: Rate of Change (ROC)\n",
        "    df['ROC'] = df['Close'].pct_change(periods=10)\n",
        "\n",
        "    #Feature 26: On-Balance Volume (OBV)\n",
        "    df['OBV'] = np.where(df['Close'] > df['Close'].shift(1), df['Volume'], -df['Volume']).cumsum()\n",
        "\n",
        "    #Feature 27: Average True Range (ATR)\n",
        "    tr = np.maximum(df['High'] - df['Low'], np.abs(df['High'] - df['Close'].shift(1)))\n",
        "    df['ATR'] = tr.rolling(window=14).mean()\n",
        "\n",
        "    #Feature 28: Chaikin Money Flow (CMF)\n",
        "    ad = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])\n",
        "    cmf_multiplier = ad * df['Volume']\n",
        "    df['CMF'] = cmf_multiplier.rolling(window=20).sum() / df['Volume'].rolling(window=20).sum()\n",
        "\n",
        "    #Feature 29: Price Rate of Change (PROC)\n",
        "    df['PROC'] = df['Close'].pct_change(periods=10)\n",
        "\n",
        "    #Feature 30: Exponential Moving Average 5\n",
        "    df['EMA_5'] = df['Close'].ewm(span=5).mean()\n",
        "\n",
        "    #Feature 31: Exponential Moving Average 10\n",
        "    df['EMA_10'] = df['Close'].ewm(span=5).mean()\n",
        "\n",
        "    #Feature 32: Exponential Moving Average 20\n",
        "    df['EMA_20'] = df['Close'].ewm(span=5).mean()\n",
        "\n",
        "    #Feature 33: Exponential Moving Average 50\n",
        "    df['EMA_50'] = df['Close'].ewm(span=5).mean()\n",
        "\n",
        "    #Feature 34: Exponential Moving Average 100\n",
        "    df['EMA_100'] = df['Close'].ewm(span=5).mean()\n",
        "\n",
        "    #Feature 35: Price-Volume Trend (PVT)\n",
        "    df['PVT'] = ((df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)) * df['Volume']\n",
        "\n",
        "    #Feature 36: Accumulation/Distribution Line (ADL)\n",
        "    df['ADL'] = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low']) * df['Volume']\n",
        "    df['ADL'] = df['ADL'].cumsum()\n",
        "\n",
        "    #Feature 37: Mass Index (MI)\n",
        "    window = 9  # You can adjust the window size as needed\n",
        "    high_low_range = df['High'] - df['Low']\n",
        "    high_low_difference = df['High'] - df['Low'].shift(1)\n",
        "    high_low_difference_ema = high_low_difference.ewm(span=window, adjust=False).mean()\n",
        "    mass_index = high_low_range / high_low_difference_ema\n",
        "    df['MI'] = mass_index.rolling(window=window).sum()\n",
        "\n",
        "    #Feature 38: Detrended Price Oscillator (DPO)\n",
        "    window = 20  # You can adjust the window size as needed\n",
        "    dpo = df['Close'] - df['Close'].rolling(window=int(window/2) + 1).mean().shift(int(window/2))\n",
        "    df['DPO'] = dpo\n",
        "\n",
        "    #Feature 39: Ulcer Index (UI)\n",
        "    window = 14  # You can adjust the window size as needed\n",
        "    returns = df['Close'].pct_change()\n",
        "    returns_squared = returns ** 2\n",
        "    rolling_max = df['Close'].rolling(window=window).max()\n",
        "    percentage_drawdown = ((df['Close'] / rolling_max) - 1) ** 2\n",
        "    ui = np.sqrt(percentage_drawdown.rolling(window=window).mean()) * 100\n",
        "    df['UI'] = ui\n",
        "\n",
        "    #Feature 40: Williams %R\n",
        "    window = 14\n",
        "    highest_high = np.max(df['High'][-window:])\n",
        "    lowest_low = np.min(df['Low'][-window:])\n",
        "    williams_r = (highest_high - df['Close']) / (highest_high - lowest_low) * -100\n",
        "    df['WilliamsR'] = williams_r\n",
        "\n",
        "    #Feature 41: Stochastic Oscillator\n",
        "    highest_high = np.max(df['High'][-window:])\n",
        "    lowest_low = np.min(df['Low'][-window:])\n",
        "    stochastic_k = (df['Close'] - lowest_low) / (highest_high - lowest_low) * 100\n",
        "    df['StochasticOscillator'] = stochastic_k\n",
        "\n",
        "    # Feature 42: Commodity Channel Index (CCI)\n",
        "    window = 20\n",
        "    typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    mean_typical_price = np.mean(typical_price.iloc[-window:])\n",
        "    mean_deviation = np.mean(np.abs(typical_price - mean_typical_price).iloc[-window:])\n",
        "    if mean_deviation != 0:\n",
        "        cci = (typical_price.iloc[-1] - mean_typical_price) / (0.015 * mean_deviation)\n",
        "    else:\n",
        "        cci = 0  # Set CCI to 0 if mean deviation is 0 to avoid divide by zero error\n",
        "    df['CCI'] = cci\n",
        "\n",
        "    #Feature 43: Average Directional Index (ADX)\n",
        "    window = 14\n",
        "    true_range = np.max([df['High'] - df['Low'], np.abs(df['High'] - df['Close'][1:]), np.abs(df['Low'] - df['Close'][1:])], axis=0)\n",
        "    smooth_true_range = np.convolve(true_range, np.ones(window)/window, mode='valid')\n",
        "    directional_movement = np.where(df['High'][1:] - df['High'][:-1] > df['Low'][:-1] - df['Low'][1:], df['High'][1:] - df['High'][:-1], df['Low'][:-1] - df['Low'][1:])\n",
        "    positive_directional_movement = np.where(directional_movement > 0, directional_movement, 0)\n",
        "    negative_directional_movement = np.where(directional_movement < 0, -directional_movement, 0)\n",
        "    smooth_positive_directional_movement = np.convolve(positive_directional_movement, np.ones(window)/window, mode='valid')\n",
        "    smooth_negative_directional_movement = np.convolve(negative_directional_movement, np.ones(window)/window, mode='valid')\n",
        "    directional_index = (smooth_positive_directional_movement / smooth_true_range) * 100\n",
        "    adx = np.mean(directional_index)\n",
        "    df['ADX'] = adx\n",
        "\n",
        "    #Feature 44: Rate of Change Ratio (ROCR)\n",
        "    window = 10\n",
        "    previous_close = df['Close'].shift(window)\n",
        "    rocr = ((df['Close'] - previous_close) / previous_close) * 100\n",
        "    df['ROCR'] = rocr\n",
        "\n",
        "\n",
        "    #Feature 45: Pivot Point\n",
        "    df['Pivot Point (PP)'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "\n",
        "    #Feature 46: Resistance 1\n",
        "    df['Resistance 1 (R1)'] = (2 * df['Pivot Point (PP)']) - df['Low']\n",
        "\n",
        "    #Feature 47: Resistance 2\n",
        "    df['Resistance 2 (R2)'] = df['Pivot Point (PP)'] + (df['High'] - df['Low'])\n",
        "\n",
        "    #Feature 48: Resistance 3\n",
        "    df['Resistance 3 (R3)'] = df['High'] + 2 * (df['Pivot Point (PP)'] - df['Low'])\n",
        "\n",
        "    #Feature 49: Support 1\n",
        "    df['Support 1 (S1)'] = (2 * df['Pivot Point (PP)']) - df['High']\n",
        "\n",
        "\n",
        "    #Feature 50: Support 2\n",
        "    df['Support 2 (S2)'] = df['Pivot Point (PP)'] - (df['High'] - df['Low'])\n",
        "\n",
        "    #Feature 51: Support 3\n",
        "    df['Support 3 (S3)'] = df['Low'] - 2 * (df['High'] - df['Pivot Point (PP)'])\n",
        "\n",
        "    #Feature 52: Ease of Movement (EOM)\n",
        "    distance_moved = (df['High'] + df['Low']) / 2 - (df['High'].shift(1) + df['Low'].shift(1)) / 2\n",
        "    box_ratio = (df['Volume'] / 100000000) / ((df['High'] - df['Low']))\n",
        "    eom = distance_moved / box_ratio\n",
        "    df['EOM'] = eom\n",
        "\n",
        "    #Feature 53: Volume Weighted Average Prie (VWAP)\n",
        "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    vwap = (tp * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
        "    df['VWAP'] = vwap\n",
        "\n",
        "    #Feature 54: Chande Momentum Oscillator (CMO)\n",
        "    window = 14\n",
        "    prev_close = df['Close'].shift(1)\n",
        "    delta = df['Close'] - prev_close\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=window).mean()\n",
        "    avg_loss = loss.rolling(window=window).mean()\n",
        "    cmo = ((avg_gain - avg_loss) / (avg_gain + avg_loss)) * 100\n",
        "    df['CMO'] = cmo\n",
        "\n",
        "    #Feature 55: Accumulative Swing Index (ASI)\n",
        "    asi = 0\n",
        "    for i in range(1, len(df)):\n",
        "        tr = max(df['High'][i] - df['Low'][i], abs(df['High'][i] - df['Close'][i-1]), abs(df['Low'][i] - df['Close'][i-1]))\n",
        "        hl = df['High'][i] - df['Low'][i]\n",
        "        hc = abs(df['High'][i] - df['Close'][i-1])\n",
        "        lc = abs(df['Low'][i] - df['Close'][i-1])\n",
        "        r = 0.1 * ((hl > hc) and (hl > lc)) * ((hl - 0.5 * hc + 0.25 * lc) / (0.001 * tr))\n",
        "        si = r * 16 * (df['Close'][i] - df['Close'][i-1]) / tr\n",
        "        asi += si\n",
        "    df['ASI'] = asi\n",
        "\n",
        "    #Feature 56: Double Exponential Moving Average 5\n",
        "    window = 5\n",
        "    ema_5 = df['Close'].ewm(span=window).mean()\n",
        "    dema_5 = 2 * ema_5 - ema_5.ewm(span=window).mean()\n",
        "    df['DEMA_5'] = dema_5\n",
        "\n",
        "    #Feature 57: Double Exponential Moving Average 10\n",
        "    window = 10\n",
        "    ema_10 = df['Close'].ewm(span=window).mean()\n",
        "    dema_10 = 2 * ema_10 - ema_10.ewm(span=window).mean()\n",
        "    df['DEMA_10'] = dema_10\n",
        "\n",
        "    #Feature 58: Double Exponential Moving Average 20\n",
        "    window = 20\n",
        "    ema_20 = df['Close'].ewm(span=window).mean()\n",
        "    dema_20 = 2 * ema_20 - ema_20.ewm(span=window).mean()\n",
        "    df['DEMA_20'] = dema_20\n",
        "\n",
        "    #Feature 59: Double Exponential Moving Average 50\n",
        "    window = 50\n",
        "    ema_50 = df['Close'].ewm(span=window).mean()\n",
        "    dema_50 = 2 * ema_50 - ema_50.ewm(span=window).mean()\n",
        "    df['DEMA_50'] = dema_50\n",
        "\n",
        "    #Feature 60: Double Exponential Moving Average 100\n",
        "    window = 100\n",
        "    ema_100 = df['Close'].ewm(span=window).mean()\n",
        "    dema_100 = 2 * ema_100 - ema_100.ewm(span=window).mean()\n",
        "    df['DEMA_100'] = dema_100\n",
        "\n",
        "    #Features 61 and 62: Keltner Channels\n",
        "    window = 20\n",
        "    avg_true_range = df['High'] - df['Low']\n",
        "    upper_band = df['Close'] + 2 * avg_true_range.rolling(window=window).mean()\n",
        "    lower_band = df['Close'] - 2 * avg_true_range.rolling(window=window).mean()\n",
        "    df['Keltner_Upper'] = upper_band\n",
        "    df['Keltner_Lower'] = lower_band\n",
        "\n",
        "    #Feature 63: Chandelier Exit Long 22\n",
        "    window = 22\n",
        "    atr = df['ATR'] # Assuming ATR (Average True Range) is already calculated\n",
        "    chandelier_exit_long = df['High'] - (3 * atr.rolling(window=window).max())\n",
        "    df['ChandelierExitLong22'] = chandelier_exit_long\n",
        "\n",
        "    #Feature 64: Chandelier Exit Short 22\n",
        "    chandelier_exit_short = df['Low'] + (3 * atr.rolling(window=window).max())\n",
        "    df['ChandelierExitShort22'] = chandelier_exit_short\n",
        "\n",
        "    #Features 65 and 66: Moving Average Envelope 5% Upper and Lower\n",
        "    ma = df['Close'].rolling(window=20).mean() # Assuming 20-day simple moving average\n",
        "    envelope_width = 0.05\n",
        "    envelope_upper = ma * (1 + envelope_width)\n",
        "    envelope_lower = ma * (1 - envelope_width)\n",
        "    df['MAE5%Upper'] = envelope_upper\n",
        "    df['MAE5%Lower'] = envelope_lower\n",
        "\n",
        "    #Feature 67: Triangular Moving Average 30\n",
        "    df['TMA_30'] = df['Close'].rolling(window=30).sum() / ((30 + 1) / 2)\n",
        "\n",
        "    #Feature 68: Triangular Exponential Moving Average 5\n",
        "    window = 5\n",
        "    ema1 = df['Close'].ewm(span=window, adjust=False).mean()\n",
        "    ema2 = ema1.ewm(span=window, adjust=False).mean()\n",
        "    ema3 = ema2.ewm(span=window, adjust=False).mean()\n",
        "    df['TEMA_5'] = 3 * ema1 - 3 * ema2 + ema3\n",
        "\n",
        "    #Feature 69: Triangular Exponential Moving Average 10\n",
        "    window = 10\n",
        "    df['TEMA_10'] = 3 * ema1 - 3 * ema2 + ema3\n",
        "\n",
        "    #Feature 70: Triangular Exponential Moving Average 20\n",
        "    window = 20\n",
        "    df['TEMA_20'] = 3 * ema1 - 3 * ema2 + ema3\n",
        "\n",
        "    #Feature 71: Triangular Exponential Moving Average 50\n",
        "    window = 50\n",
        "    df['TEMA_50'] = 3 * ema1 - 3 * ema2 + ema3\n",
        "\n",
        "    #Feature 72: Triangular Exponential Moving Average 100\n",
        "    window = 100\n",
        "    df['TEMA_100'] = 3 * ema1 - 3 * ema2 + ema3\n",
        "\n",
        "    #Feature 73: Average Price (AVP)\n",
        "    df['avp'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "\n",
        "    #Feature 74: Average Daily Range (ADR)\n",
        "    df['adr'] = df['High'] - df['Low']\n",
        "    df['adr'] = df['adr'].rolling(window=20).mean()\n",
        "\n",
        "    #Feature 75: Historical Volatility\n",
        "    df['hv'] = df['Close'].pct_change().rolling(window=20).std()\n",
        "\n",
        "    #Feature 76: High-Low Range Ratio\n",
        "    df['hl_range_ratio'] = (df['High'] - df['Low']) / df['Close']\n",
        "\n",
        "    #Feature 77: Typical Price\n",
        "    df['tp'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "\n",
        "    #Feature 78: Volume-Weighted Moving Average\n",
        "    df['vwap'] = (df['tp'] * df['Volume']).rolling(window=20).sum() / df['Volume'].rolling(window=20).sum()\n",
        "\n",
        "    #Feature 79: Daily Return\n",
        "    df['daily_return'] = df['Close'].pct_change()\n",
        "\n",
        "    #Feature 80: Average Daily Return\n",
        "    df['avg_daily_return'] = df['daily_return'].rolling(window=len(df)).mean()\n",
        "\n",
        "    #Feature 81: Rolling Volatility\n",
        "    df['Rolling Volatility'] = df['Close'].rolling(window=20).std()\n",
        "\n",
        "    #Feature 82: Price Momentum\n",
        "    df['Price Momentum'] = df['Close'].pct_change() * 100\n",
        "\n",
        "    #Feature 83: Cumulative Returns\n",
        "    df['Cumulative Returns'] = (df['Close'] / df['Close'].iloc[0] - 1) * 100\n",
        "\n",
        "    #Feature 84: Bullish Signal\n",
        "    df['Bullish Signal'] = np.where((df['MA_5'] > df['MA_10']) & (df['MA_5'].shift(1) <= df['MA_10'].shift(1)), 1, 0)\n",
        "\n",
        "    #Feature 85: Bearish Signal\n",
        "    df['Bearish Signal'] = np.where((df['MA_5'] < df['MA_10']) & (df['MA_5'].shift(1) >= df['MA_10'].shift(1)), -1, 0)\n",
        "\n",
        "    #Feature 86: Bullish Divergence\n",
        "    df['Bullish Divergence'] = np.where((rsi > rsi.shift(1)) & (rsi.shift(1) < rsi.shift(2)), 1, 0)\n",
        "\n",
        "    #Feature 87: Bearish Divergence\n",
        "    df['Bearish Divergence'] = np.where((rsi < rsi.shift(1)) & (rsi.shift(1) > rsi.shift(2)), -1, 0)\n",
        "\n",
        "    #Feature 88: Price to Moving Average Ratio\n",
        "    window = 50\n",
        "    df['Price_MA_Ratio'] = df['Close'] / df['Close'].rolling(window=window).mean()\n",
        "\n",
        "    #Feature 89: Donchian Channel High\n",
        "    window = 20\n",
        "    df['Donchian_Channel_High'] = df['High'].rolling(window=window).max()\n",
        "\n",
        "    #Feature 90: Donchian Channel Low\n",
        "    df['Donchian_Channel_Low'] = df['Low'].rolling(window=window).min()\n",
        "\n",
        "    #Feature 91: Tenkan Sen (Conversion Line)\n",
        "    df['Tenkan_Sen'] = (df['High'].rolling(window=9).max() + df['Low'].rolling(window=9).min()) / 2\n",
        "\n",
        "    #Feature 92: Kijun Sen (Base Line)\n",
        "    df['Kijun_Sen'] = (df['High'].rolling(window=26).max() + df['Low'].rolling(window=26).min()) / 2\n",
        "\n",
        "    #Feature 93: Senkoun Span A (Leading Span A)\n",
        "    df['Senkou_Span_A'] = ((df['Tenkan_Sen'] + df['Kijun_Sen']) / 2).shift(26)\n",
        "\n",
        "    #Feature 94: Senkou Span B (Leading Span B)\n",
        "    df['Senkou_Span_B'] = ((df['High'].rolling(window=52).max() + df['Low'].rolling(window=52).min()) / 2).shift(26)\n",
        "\n",
        "    #Feature 95: Chikou Span (Lagging Span)\n",
        "    df['Chikou_Span'] = df['Close'].shift(-26)\n",
        "\n",
        "    #Feature 96: Fibonacci 23.6\n",
        "    df['Fib_23.6'] = df['High'] - (df['H-L'] * 0.236)\n",
        "\n",
        "    #Feature 97: Fibonacci 38.2\n",
        "    df['Fib_38.2'] = df['High'] - (df['H-L'] * 0.382)\n",
        "\n",
        "    #Feature 98: Fibonacci 50\n",
        "    df['Fib_50.0'] = df['High'] - (df['H-L'] * 0.5)\n",
        "\n",
        "    #Feature 99: Fibonacci 61.8\n",
        "    df['Fib_61.8'] = df['High'] - (df['H-L'] * 0.618)\n",
        "\n",
        "    #Feature 100: Fibonacci 76.4\n",
        "    df['Fib_76.4'] = df['High'] - (df['H-L'] * 0.764)\n",
        "\n",
        "    # Fill NaN values with median\n",
        "    #df = df.fillna(df.median())\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkMvKUXz5s31"
      },
      "source": [
        "Apply function to all dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx6S_Jz_5uv7"
      },
      "outputs": [],
      "source": [
        "'''for i, df in enumerate(training_set):\n",
        "    training_set[i] = create_features(df)'''\n",
        "\n",
        "for i, df in enumerate(training_set):\n",
        "    create_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6c--v45wbD"
      },
      "source": [
        "Print last 5 data points for first stock dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHStpjrB50Zt"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(training_set[0].tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw7aJ4t6LQ9l"
      },
      "source": [
        "**Task 3: Training Predictive Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoQo0YaLLUeQ"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1Wekh0hMOxl"
      },
      "source": [
        "Load and preprocess the data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFduDPDPUbt"
      },
      "source": [
        "Label percent change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THsXdzvSPXIn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def label_pct_change_vectorized(pct_change, threshold1, threshold2):\n",
        "    labels = np.empty_like(pct_change)\n",
        "    labels[pct_change >= threshold2] = 1\n",
        "    labels[np.logical_and(threshold1 <= pct_change, pct_change < threshold2)] = 0\n",
        "    labels[pct_change < threshold1] = -1\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE1FcBKid4rk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply feature engineering and labeling to all DataFrames at once\n",
        "#smaller_training_set = [create_features(stock) for stock in smaller_training_set]\n",
        "targets = [label_pct_change_vectorized(stock['PCT_CHG'].values, threshold1, threshold2) for stock in training_set]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnJvqbj76f9Z"
      },
      "outputs": [],
      "source": [
        "# Split the data for each stock individually\n",
        "train_set = []\n",
        "test_set = []\n",
        "for stock, target in zip(training_set, targets):\n",
        "    stock['Target'] = target\n",
        "    X_train_stock, X_test_stock = train_test_split(stock, test_size=0.2, random_state=42)\n",
        "    train_set.append(X_train_stock)\n",
        "    test_set.append(X_test_stock)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_X1ChiA6r4B"
      },
      "outputs": [],
      "source": [
        "# Concatenate the training and testing sets separately\n",
        "X_train = pd.concat([stock.drop(columns=['Target']) for stock in train_set], ignore_index=True)\n",
        "y_train = pd.concat([stock['Target'] for stock in train_set], ignore_index=True)\n",
        "X_test = pd.concat([stock.drop(columns=['Target']) for stock in test_set], ignore_index=True)\n",
        "y_test = pd.concat([stock['Target'] for stock in test_set], ignore_index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V6IQDXuOLbZ"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Replace infinite values with NaN for target variable\n",
        "#y_train = np.where(np.isinf(y_train), np.nan, y_train)\n",
        "\n",
        "# Impute missing values for target variable\n",
        "#y_imputer = SimpleImputer(strategy='mean')\n",
        "#y_train_imputed = y_imputer.fit_transform(y_train.reshape(-1, 1)).ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORBe1rnaVdAI"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyebKj7Ccxdb"
      },
      "outputs": [],
      "source": [
        "# Define your models\n",
        "models = [\n",
        "    LogisticRegression(max_iter=1000),\n",
        "    RandomForestClassifier(),\n",
        "    RandomForestClassifier(n_estimators=50),\n",
        "    #SVC(),\n",
        "    #SVC(C=0.1),\n",
        "    MLPClassifier(),\n",
        "    MLPClassifier(hidden_layer_sizes=(50,)),\n",
        "    KNeighborsClassifier(n_neighbors=3),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    GradientBoostingClassifier(),\n",
        "    #GradientBoostingClassifier(n_estimators=100),\n",
        "    ExtraTreesClassifier(),\n",
        "    ExtraTreesClassifier(n_estimators=50),\n",
        "    # Add more models or variations of models\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9_Xe1mumwWN"
      },
      "outputs": [],
      "source": [
        "# Convert the continuous target variable into categorical labels\n",
        "def to_categorical(y, threshold_1, threshold_2):\n",
        "    if y < threshold_1:\n",
        "        return 0  # \"decrease\"\n",
        "    elif y < threshold_2:\n",
        "        return 1  # \"no big change\"\n",
        "    else:\n",
        "        return 2  # \"increase\"\n",
        "\n",
        "y_train_categorical = np.array([to_categorical(y, threshold1, threshold2) for y in y_train])\n",
        "y_test_categorical = np.array([to_categorical(y, threshold1, threshold2) for y in y_test])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isvYldmWc5Me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca10cc30-fc77-411f-c772-cce29de8134c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LogisticRegression\n",
            "Finished training LogisticRegression\n",
            "Training RandomForestClassifier\n",
            "Finished training RandomForestClassifier\n",
            "Training RandomForestClassifier\n",
            "Finished training RandomForestClassifier\n",
            "Training MLPClassifier\n",
            "Finished training MLPClassifier\n",
            "Training MLPClassifier\n",
            "Finished training MLPClassifier\n",
            "Training KNeighborsClassifier\n",
            "Finished training KNeighborsClassifier\n",
            "Training KNeighborsClassifier\n",
            "Finished training KNeighborsClassifier\n",
            "Training GradientBoostingClassifier\n",
            "Finished training GradientBoostingClassifier\n",
            "Training ExtraTreesClassifier\n",
            "Finished training ExtraTreesClassifier\n",
            "Training ExtraTreesClassifier\n",
            "Finished training ExtraTreesClassifier\n"
          ]
        }
      ],
      "source": [
        "for model in models:\n",
        "    print(f\"Training {model.__class__.__name__}\")\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train_categorical)\n",
        "    print(f\"Finished training {model.__class__.__name__}\")\n",
        "\n",
        "    # Evaluate the model on both training and validation sets\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the evaluation metrics\n",
        "    train_accuracy = accuracy_score(y_train_categorical, y_train_pred)\n",
        "    train_precision = precision_score(y_train_categorical, y_train_pred, average='weighted')\n",
        "    train_positive_pct = (y_train_pred == 1).sum() / len(y_train_pred)\n",
        "\n",
        "    val_accuracy = accuracy_score(y_test_categorical, y_val_pred)\n",
        "    val_precision = precision_score(y_test_categorical, y_val_pred, average='weighted')\n",
        "    val_positive_pct = (y_val_pred == 1).sum() / len(y_val_pred)\n",
        "\n",
        "    results.append({\n",
        "        'model': model,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'train_precision': train_precision,\n",
        "        'train_positive_pct': train_positive_pct,\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'val_precision': val_precision,\n",
        "        'val_positive_pct': val_positive_pct,\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_5LjerLdEI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44f056d-225b-45aa-f5a5-3006be04a228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(max_iter=1000):\n",
            "  Train: Accuracy=0.9935, Precision=0.9935, Positive Pct=0.3383\n",
            "  Validation: Accuracy=0.9932, Precision=0.9932, Positive Pct=0.3288\n",
            "\n",
            "RandomForestClassifier():\n",
            "  Train: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3353\n",
            "  Validation: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3270\n",
            "\n",
            "RandomForestClassifier(n_estimators=50):\n",
            "  Train: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3353\n",
            "  Validation: Accuracy=0.9998, Precision=0.9998, Positive Pct=0.3268\n",
            "\n",
            "MLPClassifier():\n",
            "  Train: Accuracy=0.9977, Precision=0.9977, Positive Pct=0.3376\n",
            "  Validation: Accuracy=0.9873, Precision=0.9875, Positive Pct=0.3333\n",
            "\n",
            "MLPClassifier(hidden_layer_sizes=(50,)):\n",
            "  Train: Accuracy=0.9997, Precision=0.9997, Positive Pct=0.3354\n",
            "  Validation: Accuracy=0.9914, Precision=0.9914, Positive Pct=0.3274\n",
            "\n",
            "KNeighborsClassifier(n_neighbors=3):\n",
            "  Train: Accuracy=0.8522, Precision=0.8540, Positive Pct=0.3344\n",
            "  Validation: Accuracy=0.6964, Precision=0.7002, Positive Pct=0.3401\n",
            "\n",
            "KNeighborsClassifier():\n",
            "  Train: Accuracy=0.8282, Precision=0.8330, Positive Pct=0.3600\n",
            "  Validation: Accuracy=0.7186, Precision=0.7277, Positive Pct=0.3660\n",
            "\n",
            "GradientBoostingClassifier():\n",
            "  Train: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3353\n",
            "  Validation: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3270\n",
            "\n",
            "ExtraTreesClassifier():\n",
            "  Train: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3353\n",
            "  Validation: Accuracy=0.9624, Precision=0.9623, Positive Pct=0.3197\n",
            "\n",
            "ExtraTreesClassifier(n_estimators=50):\n",
            "  Train: Accuracy=1.0000, Precision=1.0000, Positive Pct=0.3353\n",
            "  Validation: Accuracy=0.9578, Precision=0.9577, Positive Pct=0.3166\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"{result['model']}:\\n\"\n",
        "          f\"  Train: Accuracy={result['train_accuracy']:.4f}, Precision={result['train_precision']:.4f}, Positive Pct={result['train_positive_pct']:.4f}\\n\"\n",
        "          f\"  Validation: Accuracy={result['val_accuracy']:.4f}, Precision={result['val_precision']:.4f}, Positive Pct={result['val_positive_pct']:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGFTOkv01EOx"
      },
      "source": [
        "Task 4: Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Lwy-lb1HNC"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    ('LogisticRegression', LogisticRegression(max_iter=1000)),\n",
        "    ('RandomForestClassifier', RandomForestClassifier()),\n",
        "    ('RandomForestClassifier_50', RandomForestClassifier(n_estimators=200)),\n",
        "    #('SVC', SVC(probability=True)),\n",
        "    #('SVC_C_0.1', SVC(C=0.1, probability=True)),\n",
        "    ('MLPClassifier', MLPClassifier()),\n",
        "    ('MLPClassifier_50', MLPClassifier(hidden_layer_sizes=(100, 100))),\n",
        "    ('KNeighborsClassifier_3', KNeighborsClassifier(n_neighbors=3)),\n",
        "    ('KNeighborsClassifier_5', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('GradientBoostingClassifier', GradientBoostingClassifier()),\n",
        "    #('GradientBoostingClassifier_200', GradientBoostingClassifier(n_estimators=200)),\n",
        "    ('ExtraTreesClassifier', ExtraTreesClassifier()),\n",
        "    ('ExtraTreesClassifier_50', ExtraTreesClassifier(n_estimators=200)),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_IsFo0n1NhG"
      },
      "outputs": [],
      "source": [
        "# Set the number of features to select\n",
        "n_features_to_select = 30\n",
        "\n",
        "# Create a logistic regression estimator for RFE\n",
        "estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize the RFE selector\n",
        "selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "\n",
        "\n",
        "# Fit the selector to the training data\n",
        "selector.fit(X_train, y_train_categorical)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = selector.get_support()\n",
        "\n",
        "# Apply feature selection to the train and test sets\n",
        "X_train_selected = X_train[:, selected_features]\n",
        "X_test_selected = X_test[:, selected_features]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZKf2ydh1Oiu"
      },
      "outputs": [],
      "source": [
        "for model_name, model in models:\n",
        "    # Train the model\n",
        "    model.fit(X_train_selected, y_train_categorical)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred_train = model.predict(X_train_selected)\n",
        "    y_pred_test = model.predict(X_test_selected)\n",
        "\n",
        "    # Calculate and print the evaluation metrics\n",
        "    accuracy_train = accuracy_score(y_train_categorical, y_pred_train)\n",
        "    precision_train = precision_score(y_train_categorical, y_pred_train, average='weighted')\n",
        "    positive_pct_train = (y_pred_train == 1).sum() / len(y_pred_train)\n",
        "\n",
        "    accuracy_test = accuracy_score(y_test_categorical, y_pred_test)\n",
        "    precision_test = precision_score(y_test_categorical, y_pred_test, average='weighted')\n",
        "    positive_pct_test = (y_pred_test == 1).sum() / len(y_pred_test)\n",
        "\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Train: Accuracy={accuracy_train:.4f}, Precision={precision_train:.4f}, Positive Pct={positive_pct_train:.4f}\")\n",
        "    print(f\"  Validation: Accuracy={accuracy_test:.4f}, Precision={precision_test:.4f}, Positive Pct={positive_pct_test:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2_jrH1eLXL"
      },
      "source": [
        "Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP9gt9o7eObY"
      },
      "source": [
        "Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wgxiwXeMW9"
      },
      "outputs": [],
      "source": [
        "selected_estimators = [\n",
        "    ('lr', models[0][1]),\n",
        "    ('rf', models[1][1]),\n",
        "    ('rf_50', models[2][1]),\n",
        "    ('mlp', models[3][1]),\n",
        "    ('mlp_50', models[4][1]),\n",
        "    ('knn_3', models[5][1]),\n",
        "    ('knn_5', models[6][1]),\n",
        "    ('gbc', models[7][1]),\n",
        "    ('etc', models[8][1]),\n",
        "    ('etc_50', models[9][1]),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9G3vRQZfT2b"
      },
      "source": [
        "Blending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013LhlF2fVMO"
      },
      "source": [
        "Adaboosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB8fTz00_yls"
      },
      "outputs": [],
      "source": [
        "ensemble_methods = {\n",
        "    \"Voting\": VotingClassifier(estimators=selected_estimators, voting='soft'),\n",
        "    \"Blending\": LogisticRegression(max_iter=1000),\n",
        "    \"AdaBoosting\": AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "}\n",
        "\n",
        "best_method = None\n",
        "best_precision = 0\n",
        "evaluation_results = {}\n",
        "\n",
        "# Prepare data for Blending\n",
        "if \"Blending\" in ensemble_methods:\n",
        "    for _, model in models:  # Unpack the tuple to get the model\n",
        "        model.fit(X_train, y_train_categorical)\n",
        "    X_meta_train = np.column_stack([model.predict(X_train) for _, model in models])  # Unpack the tuple to get the model\n",
        "    X_meta_test = np.column_stack([model.predict(X_test) for _, model in models])  # Unpack the tuple to get the model\n",
        "\n",
        "for method_name, clf in ensemble_methods.items():\n",
        "    # Fit the model\n",
        "    if method_name == \"Blending\":\n",
        "        clf.fit(X_meta_train, y_train_categorical)\n",
        "    else:\n",
        "        clf.fit(X_train, y_train_categorical)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = clf.predict(X_train if method_name != \"Blending\" else X_meta_train)\n",
        "    y_test_pred = clf.predict(X_test if method_name != \"Blending\" else X_meta_test)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    train_accuracy = accuracy_score(y_train_categorical, y_train_pred)\n",
        "    train_precision = precision_score(y_train_categorical, y_train_pred, average='weighted')\n",
        "    train_positive_pct = (y_train_pred == 1).sum() / len(y_train_pred)\n",
        "\n",
        "    test_accuracy = accuracy_score(y_test_categorical, y_test_pred)\n",
        "    test_precision = precision_score(y_test_categorical, y_test_pred, average='weighted')\n",
        "    test_positive_pct = (y_test_pred == 1).sum() / len(y_test_pred)\n",
        "\n",
        "    # Store evaluation results\n",
        "    evaluation_results[method_name] = {\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'train_precision': train_precision,\n",
        "        'train_positive_pct': train_positive_pct,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_precision': test_precision,\n",
        "        'test_positive_pct': test_positive_pct\n",
        "    }\n",
        "\n",
        "    # Update best method\n",
        "    if test_precision > best_precision:\n",
        "        best_precision = test_precision\n",
        "        best_model = method_name\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{method_name} Classifier:\")\n",
        "    print(f\"  Train: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Positive Pct={train_positive_pct:.4f}\")\n",
        "    print(f\"  Test: Accuracy={test_accuracy:.4f}, Precision={test_precision:.4f}, Positive Pct={test_positive_pct:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juw2B14Lycmv"
      },
      "source": [
        "Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYTxnTM8ydmE"
      },
      "source": [
        "Load testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjD5ai-Nyeh6"
      },
      "outputs": [],
      "source": [
        "with open('testing_set1.pkl', 'rb') as f:\n",
        "    testing_set1 = pickle.load(f)\n",
        "\n",
        "with open('testing_set2.pkl', 'rb') as f:\n",
        "    testing_set2 = pickle.load(f)\n",
        "\n",
        "with open('testing_set3.pkl', 'rb') as f:\n",
        "    testing_set3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j0Fnw1gLYN6"
      },
      "outputs": [],
      "source": [
        "# Create a generator to process the data in chunks\n",
        "def process_testing_set(testing_set, create_features, label_pct_change_vectorized, threshold1, threshold2, imputer, scaler):\n",
        "    for stock in testing_set:\n",
        "        stock_df = create_features(stock)\n",
        "        stock_df['Target'] = stock_df['PCT_CHG'].apply(lambda x: label_pct_change_vectorized(x, threshold1, threshold2))\n",
        "        X_testing = stock_df.drop(columns=['Target'])\n",
        "        y_testing = stock_df['Target']\n",
        "\n",
        "        X_testing = pd.DataFrame(X_testing).replace([np.inf, -np.inf], np.nan)\n",
        "        X_testing = imputer.transform(X_testing)\n",
        "        X_testing = scaler.transform(X_testing)\n",
        "        y_testing = np.array([to_categorical(y, threshold1, threshold2) for y in y_testing])\n",
        "\n",
        "        yield X_testing, y_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW9BS4KJLLuZ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on each stock separately\n",
        "def evaluate_test_sets(model, testing_sets, create_features, label_pct_change_vectorized, threshold1, threshold2, imputer, scaler):\n",
        "    for i, testing_set in enumerate(testing_sets):\n",
        "        accuracies = []\n",
        "        precisions = []\n",
        "        positive_pcts = []\n",
        "\n",
        "        for X_testing, y_testing in process_testing_set(testing_set, create_features, label_pct_change_vectorized, threshold1, threshold2, imputer, scaler):\n",
        "            y_pred = model.predict(X_testing)\n",
        "            accuracy = accuracy_score(y_testing, y_pred)\n",
        "            precision = precision_score(y_testing, y_pred, average='weighted')\n",
        "            positive_pct = (y_pred == 1).sum() / len(y_pred)\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            precisions.append(precision)\n",
        "            positive_pcts.append(positive_pct)\n",
        "\n",
        "        print(f\"Testing set {i + 1}:\")\n",
        "        print(f\"Accuracy={np.mean(accuracies):.4f}, Precision={np.mean(precisions):.4f}, Positive Pct={np.mean(positive_pcts):.4f}\\n\")\n",
        "\n",
        "        # Clear memory\n",
        "        del testing_set, accuracies, precisions, positive_pcts\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp_68kzNLMj6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on both testing sets\n",
        "evaluate_test_sets(ensemble_methods[best_model], [testing_set1, testing_set2, testing_set3], create_features, label_pct_change_vectorized, threshold1, threshold2, imputer, scaler)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}